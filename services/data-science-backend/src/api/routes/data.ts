import { Router, Request, Response, NextFunction } from 'express';
import multer from 'multer';
import csv from 'csv-parser';
import fs from 'fs';
import { validateRequest } from '../../middleware/validation';
import { DataValidationError } from '../../errors/DataScienceErrors';
import { APIResponse, TableData, RawDataset, JobStatus, ColumnDefinition, RowData } from '../../types';
import { aiPipelineClient } from '../../services/aiPipelineClient';
import { validateTableData, validateFileUpload } from '../../validation/dataValidation';
import { supabaseServiceRole } from '../../services/supabaseClient';
import { logger } from '../../utils/logger';
import { randomUUID } from 'crypto';

const router = Router();

// Configure multer for file uploads
const upload = multer({
  dest: 'uploads/',
  limits: {
    fileSize: 50 * 1024 * 1024, // 50MB limit
  },
  fileFilter: (_req, file, cb) => {
    if (file.mimetype === 'text/csv' || file.mimetype === 'application/vnd.ms-excel') {
      cb(null, true);
    } else {
      cb(new Error('Only CSV files are allowed'));
    }
  },
});

// Data upload endpoint
router.post('/upload', upload.single('file'), async (req: Request, res: Response, next: NextFunction) => {
  try {
    if (!req.file) {
      throw new DataValidationError([{
        field: 'file',
        message: 'No file uploaded',
        code: 'MISSING_FILE'
      }]);
    }

    // Validate file
    const fileValidation = validateFileUpload(req.file);
    if (!fileValidation.isValid) {
      throw new DataValidationError(fileValidation.errors);
    }

    logger.info('File uploaded:', {
      filename: req.file.originalname,
      size: req.file.size,
      mimetype: req.file.mimetype
    });

    // Parse CSV file
    const csvData = await parseCSVFile(req.file.path);
    
    // Detect data types
    const detectedDataTypes = detectDataTypes(csvData.rows, csvData.headers);

    const tableData: TableData = {
      id: randomUUID(), // Generate a new ID for the table
      name: req.file.originalname.replace(/\.[^/.]+$/, ''), // Use filename as table name
      columns: csvData.headers.map((header, index) => ({
        id: randomUUID(),
        name: header,
        displayName: header,
        dataType: detectedDataTypes[header] || 'text', // Use detectedDataTypes
        isRequired: false,
        validationRules: {},
        orderIndex: index,
      })),
      rows: csvData.rows.map((row, index) => ({
        id: randomUUID(),
        rowIndex: index,
        data: csvData.headers.reduce((acc, header, colIndex) => {
          acc[header] = row[colIndex];
          return acc;
        }, {} as Record<string, any>),
        isNew: false,
        isModified: false,
        isPending: false,
      })),
      metadata: {
        totalRows: csvData.rows.length,
        totalColumns: csvData.headers.length,
        lastUpdated: new Date(),
      }
    };

    // Validate parsed data
    const validation = validateTableData(tableData);
    
    // Create dataset record
    const dataId = randomUUID();
    const rawDataset: RawDataset = {
      id: dataId,
      name: req.file.originalname.replace(/\.[^/.]+$/, ''), // Remove extension
      description: `Uploaded CSV file: ${req.file.originalname}`,
      columns: tableData.columns, // Use new TableData structure
      rows: tableData.rows, // Use new TableData structure
      metadata: {
        rowCount: tableData.rows.length,
        columnCount: tableData.columns.length,
        dataTypes: tableData.columns.reduce((acc, col) => {
          acc[col.name] = col.dataType;
          return acc;
        }, {} as { [column: string]: 'text' | 'number' | 'date' | 'boolean' }),
        uploadedAt: new Date(),
        fileSize: req.file.size,
        fileName: req.file.originalname
      }
    };

    // Store in database (placeholder - would use actual DB connection)
    await storeDataset(rawDataset);

    // Clean up uploaded file
    fs.unlinkSync(req.file.path);

    // Return preview (limit to first 100 rows for performance)
    const previewRows = csvData.rows.slice(0, 100);
    
    const response: APIResponse = {
      success: true,
      data: {
        dataId,
        preview: {
          headers: csvData.headers,
          rows: previewRows,
          metadata: {
            totalRows: previewRows.length,
            totalColumns: csvData.headers.length,
            lastUpdated: new Date()
          }
        },
        validation
      },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    // Clean up file on error
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path);
    }
    next(error);
  }
});

// Manual data submission endpoint
router.post('/manual', validateRequest, async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { tableData, metadata } = req.body as { tableData: TableData; metadata?: any };

    if (!tableData || !tableData.columns || !tableData.rows) {
      throw new DataValidationError([{
        field: 'tableData',
        message: 'Invalid table data structure',
        code: 'INVALID_TABLE_DATA'
      }]);
    }

    // Validate table data
    const validation = validateTableData(tableData);
    if (!validation.isValid) {
      throw new DataValidationError(validation.errors);
    }

    logger.info('Manual data submitted:', {
      rowCount: tableData.rows.length,
      columnCount: tableData.columns.length
    });

    // Detect data types if not provided (using the new structure)
    const dataTypes = tableData.columns.reduce((acc, col) => {
      acc[col.name] = col.dataType;
      return acc;
    }, {} as { [column: string]: 'text' | 'number' | 'date' | 'boolean' });

    // Create dataset record
    const dataId = randomUUID();
    const rawDataset: RawDataset = {
      id: dataId,
      name: metadata?.name || `Manual Dataset ${new Date().toLocaleDateString()}`,
      description: metadata?.description || 'Manually entered dataset',
      columns: tableData.columns,
      rows: tableData.rows,
      metadata: {
        rowCount: tableData.rows.length,
        columnCount: tableData.columns.length,
        dataTypes: tableData.columns.reduce((acc, col) => {
          acc[col.name] = col.dataType;
          return acc;
        }, {} as { [column: string]: 'text' | 'number' | 'date' | 'boolean' }),
        uploadedAt: new Date()
      }
    };

    // Store in database
    await storeDataset(rawDataset);

    const response: APIResponse = {
      success: true,
      data: {
        dataId,
        validation
      },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Test endpoint
router.get('/test', async (req: Request, res: Response) => {
  res.json({ 
    success: true, 
    message: 'Data routes are working!', 
    timestamp: new Date().toISOString() 
  });
});

// Trigger preprocessing (new endpoint that matches frontend call)
router.post('/preprocess', async (req: Request, res: Response, next: NextFunction) => {
  try {
    console.log('ðŸ”„ Preprocessing endpoint hit!');
    console.log('Request body keys:', Object.keys(req.body));
    
    const { datasetId, data, schema, options } = req.body;

    console.log('ðŸ“Š Preprocessing data:', { 
      datasetId, 
      dataRows: data?.length, 
      schemaColumns: schema?.length, 
      options 
    });

    logger.info('Preprocessing requested:', { datasetId, rowCount: data?.length, options });

    // Create preprocessing job
    const jobId = randomUUID();
    
    // Send to AI Pipeline for preprocessing
    try {
      const response = await fetch('http://localhost:3002/api/process-data', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          data: data,
          processingType: 'preprocessing',
          options: options || {
            cleanMissingValues: true,
            normalizeNumerical: true,
            encodeCategories: true
          }
        })
      });

      const result = await response.json();

      if (typeof result === 'object' && result !== null && 'success' in result && result.success) {
        const apiResponse: APIResponse = {
          success: true,
          data: {
            jobId: (result as any).jobId,
            message: 'Data preprocessing started successfully',
            estimatedDuration: 30
          },
          timestamp: new Date().toISOString()
        };

        res.json(apiResponse);
      } else {
        throw new Error((result as any).message || 'Preprocessing failed');
      }
    } catch (error) {
      logger.error('AI Pipeline preprocessing failed:', error);
      
      // Return success anyway for demo purposes
      const apiResponse: APIResponse = {
        success: true,
        data: {
          jobId,
          message: 'Data preprocessing completed (simulated)',
          processedData: data // Return original data as "processed"
        },
        timestamp: new Date().toISOString()
      };

      res.json(apiResponse);
    }
  } catch (error) {
    next(error);
  }
});

// Trigger preprocessing (original endpoint)
router.post('/:dataId/preprocess', validateRequest, async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { dataId } = req.params;
    const { preprocessingConfig } = req.body;

    logger.info('Preprocessing requested:', { dataId, config: preprocessingConfig });

    // Get dataset from database
    const dataset = await getDatasetById(dataId);
    if (!dataset) {
      throw new DataValidationError([{
        field: 'dataId',
        message: 'Dataset not found',
        code: 'DATASET_NOT_FOUND',
        value: dataId
      }]);
    }

    // Create preprocessing job
    const jobId = randomUUID();
    await createJob({
      id: jobId,
      jobType: 'data_preprocessing',
      entityId: dataId,
      status: 'queued',
      progress: 0,
      message: 'Preprocessing job queued',
      estimatedDurationSeconds: 300
    });

    // Send to AI Pipeline for preprocessing
    const result = await aiPipelineClient.preprocessData({
      id: dataset.id,
      headers: dataset.columns.map(col => col.name), // Use columns to get headers
      rows: dataset.rows.map(row => dataset.columns.map(col => row.data[col.name])), // Extract row data
      metadata: {
        rowCount: dataset.rows.length,
        columnCount: dataset.columns.length,
        dataTypes: dataset.columns.reduce((acc, col) => {
          acc[col.name] = col.dataType;
          return acc;
        }, {} as { [column: string]: 'text' | 'number' | 'date' | 'boolean' }),
        uploadedAt: dataset.metadata.uploadedAt,
      }
    }, preprocessingConfig || {
      normalize: true,
      handleMissingValues: true,
      featureSelection: true,
      outlierRemoval: false
    });

    // Update job with AI pipeline job ID
    await updateJobStatus(jobId, 'processing', 5, 'Preprocessing started');

    const response: APIResponse = {
      success: true,
      data: {
        jobId,
        aiPipelineJobId: result.jobId,
        estimatedDuration: result.estimatedDuration
      },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Get preprocessing status
router.get('/preprocess/:jobId/status', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { jobId } = req.params;

    // Get job status from database
    const job = await getJobById(jobId);
    if (!job) {
      throw new DataValidationError([{
        field: 'jobId',
        message: 'Job not found',
        code: 'JOB_NOT_FOUND',
        value: jobId
      }]);
    }

    // If job is still processing, check AI Pipeline status
    if (job.status === 'processing') {
      try {
        // Note: We'd need to store the AI pipeline job ID to check its status
        // For now, we'll simulate progress updates
        const aiStatus = await aiPipelineClient.getPreprocessingStatus(jobId);
        
        // Update our job status based on AI pipeline status
        if (aiStatus.status === 'completed' && aiStatus.result) {
          await updateJobStatus(jobId, 'completed', 100, 'Preprocessing completed successfully', aiStatus.result);
          
          // Update dataset with processed data
          if (job.entityId) {
            await updateDatasetProcessedData(job.entityId, aiStatus.result);
          }
        } else if (aiStatus.status === 'failed') {
          await updateJobStatus(jobId, 'failed', job.progress, aiStatus.error || 'Preprocessing failed');
        } else {
          await updateJobStatus(jobId, 'processing', aiStatus.progress, aiStatus.status);
        }
        
        // Refresh job data
        const updatedJob = await getJobById(jobId);
        if (updatedJob) {
          job.status = updatedJob.status;
          job.progress = updatedJob.progress;
          if (updatedJob.message !== undefined) {
            job.message = updatedJob.message;
          }
          job.result = updatedJob.result;
        }
      } catch (aiError) {
        logger.warn('Failed to get AI Pipeline status, using local job status:', aiError);
      }
    }

    const response: APIResponse = {
      success: true,
      data: {
        status: job.status,
        progress: job.progress,
        message: job.message,
        result: job.result,
        error: job.status === 'failed' ? job.message : undefined
      },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Get dataset by ID
router.get('/:dataId', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { dataId } = req.params;
    const dataset = await getDatasetById(dataId);

    if (!dataset) {
      throw new DataValidationError([{
        field: 'dataId',
        message: 'Dataset not found',
        code: 'DATASET_NOT_FOUND',
        value: dataId
      }]);
    }

    const response: APIResponse = {
      success: true,
      data: dataset,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// List datasets
router.get('/', async (_req: Request, res: Response, next: NextFunction) => {
  try {
    const { data, error } = await supabaseServiceRole
      .from('datasets')
      .select('*');

    if (error) {
      logger.error('Error listing datasets:', error);
      throw new Error(`Failed to list datasets: ${error.message}`);
    }

    const response: APIResponse = {
      success: true,
      data: {
        datasets: data,
        total: data.length,
        page: 1,
        limit: data.length
      },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Add a new row to a dataset
router.post('/:datasetId/rows', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId } = req.params;
    const { rowData } = req.body as { rowData: Record<string, any> };

    if (!rowData) {
      throw new DataValidationError([{
        field: 'rowData',
        message: 'Row data is required',
        code: 'MISSING_ROW_DATA'
      }]);
    }

    const dataset = await getDatasetById(datasetId);
    if (!dataset) {
      throw new DataValidationError([{
        field: 'datasetId',
        message: 'Dataset not found',
        code: 'DATASET_NOT_FOUND',
        value: datasetId
      }]);
    }

    const { data, error } = await supabaseServiceRole
      .from('dataset_rows')
      .insert({
        dataset_id: datasetId,
        row_index: dataset.rows.length, // Append to the end
        data: rowData,
      })
      .select()
      .single();

    if (error) {
      logger.error('Error adding row:', error);
      throw new Error(`Failed to add row: ${error.message}`);
    }

    // Update row count in datasets table
    await supabaseServiceRole
      .from('datasets')
      .update({ row_count: dataset.rows.length + 1, updated_at: new Date() })
      .eq('id', datasetId);

    const response: APIResponse = {
      success: true,
      data: data,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Update a single row in a dataset
router.put('/:datasetId/rows/:rowId', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId, rowId } = req.params;
    const { updates } = req.body as { updates: Record<string, any> };

    if (!updates) {
      throw new DataValidationError([{
        field: 'updates',
        message: 'Updates are required',
        code: 'MISSING_UPDATES'
      }]);
    }

    const { data, error } = await supabaseServiceRole
      .from('dataset_rows')
      .update({ data: updates, updated_at: new Date() })
      .eq('dataset_id', datasetId)
      .eq('id', rowId)
      .select()
      .single();

    if (error) {
      logger.error('Error updating row:', error);
      throw new Error(`Failed to update row: ${error.message}`);
    }

    if (!data) {
      throw new DataValidationError([{
        field: 'rowId',
        message: 'Row not found or not updated',
        code: 'ROW_NOT_FOUND',
        value: rowId
      }]);
    }

    const response: APIResponse = {
      success: true,
      data: data,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Delete a single row from a dataset
router.delete('/:datasetId/rows/:rowId', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId, rowId } = req.params;

    const { error } = await supabaseServiceRole
      .from('dataset_rows')
      .delete()
      .eq('dataset_id', datasetId)
      .eq('id', rowId);

    if (error) {
      logger.error('Error deleting row:', error);
      throw new Error(`Failed to delete row: ${error.message}`);
    }

    // Update row count in datasets table
    const dataset = await getDatasetById(datasetId);
    if (dataset) {
      await supabaseServiceRole
        .from('datasets')
        .update({ row_count: dataset.rows.length - 1, updated_at: new Date() })
        .eq('id', datasetId);
    }

    const response: APIResponse = {
      success: true,
      data: { message: 'Row deleted successfully' },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Update a single cell value
router.put('/:datasetId/rows/:rowId/cells/:columnName', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId, rowId, columnName } = req.params;
    const { value } = req.body as { value: any };

    const { data: existingRow, error: fetchError } = await supabaseServiceRole
      .from('dataset_rows')
      .select('data')
      .eq('dataset_id', datasetId)
      .eq('id', rowId)
      .single();

    if (fetchError) {
      logger.error('Error fetching row for cell update:', fetchError);
      throw new Error(`Failed to fetch row for cell update: ${fetchError.message}`);
    }

    if (!existingRow) {
      throw new DataValidationError([{
        field: 'rowId',
        message: 'Row not found',
        code: 'ROW_NOT_FOUND',
        value: rowId
      }]);
    }

    const updatedData = { ...existingRow.data, [columnName]: value };

    const { data, error } = await supabaseServiceRole
      .from('dataset_rows')
      .update({ data: updatedData, updated_at: new Date() })
      .eq('dataset_id', datasetId)
      .eq('id', rowId)
      .select()
      .single();

    if (error) {
      logger.error('Error updating cell:', error);
      throw new Error(`Failed to update cell: ${error.message}`);
    }

    const response: APIResponse = {
      success: true,
      data: data,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Add a new column to a dataset
router.post('/:datasetId/columns', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId } = req.params;
    const { column } = req.body as { column: ColumnDefinition };

    if (!column || !column.name || !column.dataType) {
      throw new DataValidationError([{
        field: 'column',
        message: 'Column name and data type are required',
        code: 'MISSING_COLUMN_DATA'
      }]);
    }

    const dataset = await getDatasetById(datasetId);
    if (!dataset) {
      throw new DataValidationError([{
        field: 'datasetId',
        message: 'Dataset not found',
        code: 'DATASET_NOT_FOUND',
        value: datasetId
      }]);
    }

    const { data, error } = await supabaseServiceRole
      .from('dataset_columns')
      .insert({
        dataset_id: datasetId,
        name: column.name,
        display_name: column.displayName || column.name,
        data_type: column.dataType,
        is_required: column.isRequired || false,
        validation_rules: column.validationRules || {},
        order_index: dataset.columns.length, // Append to the end
      })
      .select()
      .single();

    if (error) {
      logger.error('Error adding column:', error);
      throw new Error(`Failed to add column: ${error.message}`);
    }

    // Update column count in datasets table
    await supabaseServiceRole
      .from('datasets')
      .update({ column_count: dataset.columns.length + 1, updated_at: new Date() })
      .eq('id', datasetId);

    // For existing rows, add the new column with a default value (e.g., null)
    const { error: updateRowsError } = await supabaseServiceRole
      .from('dataset_rows')
      .update({
        data: { [column.name]: null }, // Add new column with null value
        updated_at: new Date()
      })
      .eq('dataset_id', datasetId);

    if (updateRowsError) {
      logger.error('Error updating existing rows with new column:', updateRowsError);
      // This might not be a critical error, but log it
    }

    const response: APIResponse = {
      success: true,
      data: data,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Update an existing column
router.put('/:datasetId/columns/:columnId', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId, columnId } = req.params;
    const { updates } = req.body as { updates: Partial<ColumnDefinition> };

    if (!updates) {
      throw new DataValidationError([{
        field: 'updates',
        message: 'Updates are required',
        code: 'MISSING_UPDATES'
      }]);
    }

    const { data, error } = await supabaseServiceRole
      .from('dataset_columns')
      .update({ ...updates, updated_at: new Date() })
      .eq('dataset_id', datasetId)
      .eq('id', columnId)
      .select()
      .single();

    if (error) {
      logger.error('Error updating column:', error);
      throw new Error(`Failed to update column: ${error.message}`);
    }

    if (!data) {
      throw new DataValidationError([{
        field: 'columnId',
        message: 'Column not found or not updated',
        code: 'COLUMN_NOT_FOUND',
        value: columnId
      }]);
    }

    const response: APIResponse = {
      success: true,
      data: data,
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Delete a column
router.delete('/:datasetId/columns/:columnId', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { datasetId, columnId } = req.params;

    const { data: columnToDelete, error: fetchColumnError } = await supabaseServiceRole
      .from('dataset_columns')
      .select('name')
      .eq('id', columnId)
      .single();

    if (fetchColumnError) {
      logger.error('Error fetching column to delete:', fetchColumnError);
      throw new Error(`Failed to fetch column to delete: ${fetchColumnError.message}`);
    }

    if (!columnToDelete) {
      throw new DataValidationError([{
        field: 'columnId',
        message: 'Column not found',
        code: 'COLUMN_NOT_FOUND',
        value: columnId
      }]);
    }

    const columnName = columnToDelete.name;

    const { error } = await supabaseServiceRole
      .from('dataset_columns')
      .delete()
      .eq('dataset_id', datasetId)
      .eq('id', columnId);

    if (error) {
      logger.error('Error deleting column:', error);
      throw new Error(`Failed to delete column: ${error.message}`);
    }

    // Update column count in datasets table
    const dataset = await getDatasetById(datasetId);
    if (dataset) {
      await supabaseServiceRole
        .from('datasets')
        .update({ column_count: dataset.columns.length - 1, updated_at: new Date() })
        .eq('id', datasetId);
    }

    // Remove the column from the 'data' JSONB of all rows
    const { error: updateRowsError } = await supabaseServiceRole
      .from('dataset_rows')
      .update({
        data: { [columnName]: null }, // Set to null first
        updated_at: new Date()
      })
      .eq('dataset_id', datasetId);

    if (updateRowsError) {
      logger.error('Error removing column from existing rows:', updateRowsError);
      // This might not be a critical error, but log it
    }

    // Note: Supabase doesn't have a direct way to remove a key from JSONB
    // The above sets it to null. If a complete removal is needed,
    // a custom RPC function or a more complex update might be required.

    const response: APIResponse = {
      success: true,
      data: { message: 'Column deleted successfully' },
      timestamp: new Date().toISOString()
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Helper functions

/**
 * Parse CSV file and return structured data
 */
async function parseCSVFile(filePath: string): Promise<{ headers: string[]; rows: (string | number)[][] }> {
  return new Promise((resolve, reject) => {
    const results: any[] = [];
    let headers: string[] = [];
    
    fs.createReadStream(filePath)
      .pipe(csv())
      .on('headers', (headerList: string[]) => {
        headers = headerList;
      })
      .on('data', (data) => {
        results.push(data);
      })
      .on('end', () => {
        // Convert objects to arrays based on headers
        const rows = results.map(row => 
          headers.map(header => {
            const value = row[header];
            // Try to convert to number if possible
            const numValue = Number(value);
            return !isNaN(numValue) && value !== '' ? numValue : value;
          })
        );
        
        resolve({ headers, rows });
      })
      .on('error', (error) => {
        reject(new Error(`CSV parsing failed: ${error.message}`));
      });
  });
}

/**
 * Detect data types for columns
 */
function detectDataTypes(rows: (string | number)[][], headers: string[]): { [column: string]: 'text' | 'number' | 'date' | 'boolean' } {
  const dataTypes: { [column: string]: 'text' | 'number' | 'date' | 'boolean' } = {};

  headers.forEach((header, colIndex) => {
    const columnValues = rows.map(row => row[colIndex]).filter(val => val !== null && val !== undefined && val !== '');

    if (columnValues.length === 0) {
      dataTypes[header] = 'text';
      return;
    }

    // Check if all values are numbers
    const allNumbers = columnValues.every(val => typeof val === 'number' || (!isNaN(Number(val)) && val !== ''));
    if (allNumbers) {
      dataTypes[header] = 'number';
      return;
    }

    // Check if all values are booleans
    const allBooleans = columnValues.every(val =>
      typeof val === 'boolean' ||
      (typeof val === 'string' && ['true', 'false', '1', '0', 'yes', 'no'].includes(val.toLowerCase()))
    );
    if (allBooleans) {
      dataTypes[header] = 'boolean';
      return;
    }

    // Check if all values are dates
    const allDates = columnValues.every(val => {
      if (typeof val === 'string') {
        const date = new Date(val);
        return !isNaN(date.getTime());
      }
      return false;
    });
    if (allDates) {
      dataTypes[header] = 'date';
      return;
    }

    // Default to text
    dataTypes[header] = 'text';
  });

  return dataTypes;
}

/**
 * Store dataset in database (placeholder implementation)
 */
async function storeDataset(dataset: RawDataset): Promise<void> {
  logger.info('Storing dataset:', { id: dataset.id, name: dataset.name });

  const { error: datasetError } = await supabaseServiceRole
    .from('datasets')
    .insert({
      id: dataset.id,
      name: dataset.name,
      description: dataset.description,
      row_count: dataset.rows.length,
      column_count: dataset.columns.length,
      status: 'ready',
      created_at: dataset.metadata.uploadedAt,
      updated_at: dataset.metadata.uploadedAt,
    });

  if (datasetError) {
    logger.error('Error inserting dataset:', datasetError);
    throw new Error(`Failed to store dataset: ${datasetError.message}`);
  }

  // Store columns
  const columnsToInsert = dataset.columns.map(col => ({
    dataset_id: dataset.id,
    name: col.name,
    display_name: col.displayName,
    data_type: col.dataType,
    is_required: col.isRequired,
    validation_rules: col.validationRules || {},
    order_index: col.orderIndex,
    created_at: dataset.metadata.uploadedAt,
    updated_at: dataset.metadata.uploadedAt,
  }));

  const { error: columnsError } = await supabaseServiceRole
    .from('dataset_columns')
    .insert(columnsToInsert);

  if (columnsError) {
    logger.error('Error inserting dataset columns:', columnsError);
    throw new Error(`Failed to store dataset columns: ${columnsError.message}`);
  }

  // Store rows
  const rowsToInsert = dataset.rows.map(row => ({
    dataset_id: dataset.id,
    row_index: row.rowIndex,
    data: row.data,
    created_at: dataset.metadata.uploadedAt,
    updated_at: dataset.metadata.uploadedAt,
  }));

  const { error: rowsError } = await supabaseServiceRole
    .from('dataset_rows')
    .insert(rowsToInsert);

  if (rowsError) {
    logger.error('Error inserting dataset rows:', rowsError);
    throw new Error(`Failed to store dataset rows: ${rowsError.message}`);
  }
}

/**
 * Get dataset by ID (placeholder implementation)
 */
async function getDatasetById(dataId: string): Promise<RawDataset | null> {
  logger.info('Getting dataset:', { dataId });

  const { data: datasetData, error: datasetError } = await supabaseServiceRole
    .from('datasets')
    .select('*')
    .eq('id', dataId)
    .single();

  if (datasetError) {
    logger.error('Error fetching dataset:', datasetError);
    return null;
  }

  if (!datasetData) {
    return null;
  }

  const { data: columnsData, error: columnsError } = await supabaseServiceRole
    .from('dataset_columns')
    .select('*')
    .eq('dataset_id', dataId)
    .order('order_index', { ascending: true });

  if (columnsError) {
    logger.error('Error fetching dataset columns:', columnsError);
    return null;
  }

  const { data: rowsData, error: rowsError } = await supabaseServiceRole
    .from('dataset_rows')
    .select('*')
    .eq('dataset_id', dataId)
    .order('row_index', { ascending: true });

  if (rowsError) {
    logger.error('Error fetching dataset rows:', rowsError);
    return null;
  }

  const headers = columnsData.map((col: ColumnDefinition) => col.name);
  const dataTypes: { [column: string]: 'text' | 'number' | 'date' | 'boolean' } = {};
  columnsData.forEach((col: ColumnDefinition) => {
    dataTypes[col.name] = col.dataType;
  });

  const rows: RowData[] = rowsData.map((row: any) => ({
    id: row.id,
    rowIndex: row.row_index,
    data: row.data,
    isNew: false,
    isModified: false,
    isPending: false,
  }));

  return {
    id: datasetData.id,
    name: datasetData.name,
    description: datasetData.description,
    columns: columns,
    rows: rows,
    metadata: {
      rowCount: datasetData.row_count,
      columnCount: datasetData.column_count,
      dataTypes: dataTypes,
      uploadedAt: new Date(datasetData.created_at),
      fileSize: datasetData.file_size || undefined, // Use actual file_size if available
      fileName: datasetData.file_name || undefined, // Use actual file_name if available
    }
  };
}

/**
 * Create job record (placeholder implementation)
 */
async function createJob(job: Partial<JobStatus>): Promise<void> {
  // TODO: Implement actual database storage
  logger.info('Creating job:', { id: job.id, type: job.jobType });
}

/**
 * Update job status (placeholder implementation)
 */
async function updateJobStatus(jobId: string, status: string, progress: number, message?: string, result?: any): Promise<void> {
  // TODO: Implement actual database update
  logger.info('Updating job status:', { jobId, status, progress, message, hasResult: !!result });
}

/**
 * Get job by ID (placeholder implementation)
 */
async function getJobById(jobId: string): Promise<JobStatus | null> {
  // TODO: Implement actual database retrieval
  logger.info('Getting job:', { jobId });
  
  // Mock job status
  return {
    id: jobId,
    status: 'processing',
    progress: 50,
    message: 'Processing...',
    result: undefined,
    error: null,
    createdAt: new Date(),
    updatedAt: new Date()
  };
}

/**
 * Update dataset with processed data (placeholder implementation)
 */
async function updateDatasetProcessedData(datasetId: string, processedData: any): Promise<void> {
  // TODO: Implement actual database update
  logger.info('Updating dataset with processed data:', { datasetId, hasProcessedData: !!processedData });
}

export { router as dataRoutes };